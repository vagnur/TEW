# -*- coding: utf-8 -*-
import urlparse
from selenium import webdriver
from bs4 import BeautifulSoup

browser = webdriver.Firefox()
#Se encuenta la cantidad máxima de páginas que tiene el genero
browser.get('http://www.metacritic.com/browse/albums/genre/date/rb')
page_source = BeautifulSoup(browser.page_source,'lxml')
page_max = page_source.find_all('a',class_='page_num')
page_number = 1
for data in page_max:
	page_number = int(data.get_text())
#Se encuentran los links a cada álbum
for i in range(page_number):
	browser.get('http://www.metacritic.com/browse/albums/genre/date/rb?page='+str(i))
	page_source = BeautifulSoup(browser.page_source,'lxml')
	albums = page_source.find_all('li',class_='product')
	for source in albums:
		for link in source.find_all('a'):
			#Info del álbum : Nombre y autor
			info = str(link.get('href'))
			info = info.strip()
			info = info.split('/')
			with open('discos','a') as discos:
				discos.write(str(info[2])+','+str(info[3])+',')
			browser.get('http://www.metacritic.com'+link.get('href'))
			#Se accede al álbum
			page_source=BeautifulSoup(browser.page_source,'lxml')
			#Puntuación de la crítica
			meta_score=page_source.find('div',class_='metascore_w xlarge album positive')
			with open('discos','a') as discos:
				discos.write(meta_score.get_text()+',')
			#Puntuación de los usuarios
			user_score=page_source.find('div',class_='metascore_w user large album positive')
			with open('discos','a') as discos:
				discos.write(str(user_score.get_text())+',')
			#Fecha publicación álbum
			release = page_source.find('span',itemprop='datePublished')
			release = release.get_text().strip()
			with open('discos','a') as discos:
				discos.write(release+'\n')
			#Se accede a los comentarios de los críticos ### SOLO SE SACAN LOS MEJORES 100
			browser.get('http://www.metacritic.com'+link.get('href')+'/critic-reviews')
			page_source=BeautifulSoup(browser.page_source,'lxml')
			comments_section = page_source.find('ol',class_='reviews critic_reviews')
			try:
				for source in comments_section.find_all('div',class_='review_content'):
					score = source.find('div',class_='metascore_w')
					score = score.get_text().strip()
					comentario = source.find('div',class_='review_body')
					comentario = comentario.get_text().encode('ascii', 'ignore').strip()
					with open('experts','a') as expert:
						expert.write(score+','+'"'+comentario.replace('\"','\'')+'"'+'\n')
			except:
				print 'No hay datos'
			#Se accede a los comentarios de los usuarios ### SOLO SE SACAN LOS MEJORES 100
			browser.get('http://www.metacritic.com'+link.get('href')+'/user-reviews')
			page_source=BeautifulSoup(browser.page_source,'lxml')
			comments_section = page_source.find('ol',class_='reviews user_reviews')
			try:
				for source in comments_section.find_all('div',class_='review_top_l'):
					score = source.find('div',class_='metascore_w')
					score = score.get_text().strip()
					comentario = source.find('div',class_='review_body')
					if comentario.find('span',class_='blurb blurb_expanded'):
						comentario = comentario.find('span',class_='blurb blurb_expanded')
					comentario = comentario.get_text().encode('ascii','ignore').strip()
					ups = source.find('span',class_='total_ups').get_text().strip()
					downs = source.find('span',class_='total_thumbs').get_text().strip()
					date = source.find('div',class_='date').get_text().strip()
					with open('people','a') as people:
						people.write(score+','+'"'+comentario.replace('\"','\'')+'"'+','+ups+','+downs+','+'"'+date+'"'+'\n')			
			except:
				print 'No hay datos'
		with open('experts','a') as expert:
			expert.write('\n')
		with open('people','a') as people:
			people.write('\n')
		with open('log.dat','a') as log:
			log.write('Info extraida satisfactoriamente de: '+str(info[2])+str(info[3])+'\n')
browser.close()

